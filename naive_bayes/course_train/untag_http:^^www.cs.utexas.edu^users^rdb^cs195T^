


CS195T: Introduction to Graduate Computer Science







CS195T:Introduction to Graduate Computer Science
Fall 1996 (47865)


 Lecture: Monday, 12:00 - 1:00, in TAY 2.106.
 Instructor: Robert Blumofe

 Office: Taylor 4.118
 Phone: 471-9557
 Email: rdb@cs.utexas.edu
 Office hours: Thursday 1:30 - 3:30,
but feel free to stop by any time.










This is a one-semester seminar course that can only be taken on a
pass/fail basis. For graduate students, the course is CS 195T:
Introduction to Graduate Computer Science. For undergraduates,
the course is CS 178: Undergraduate Topics in Computer Science
(Honors) (47730).  To receive credit for the course, a student must be
registered for the course, and attend at least 11 of the 13 lectures.




Schedule



Speaker
Title



September 9
Dan Miranker
Alamo: The Net as a Data Warehouse



September 16
Ben Kuipers
The Spatial Semantic Hierarchy for Human
and Robot Cognitive Maps



September 23
Robert Blumofe
Cilk and Cilk-NOW: Adaptive and Reliable
Parallel Computing on Networks of Workstations



September 30
Risto Miikkulainen
Learning Sequential Decision Tasks Through
Symbiotic Evolution of Neural Networks



October 7
Vladimir Lifschitz
Mathematical Principles of Logic Programming



October 14
Paul Wilson
Extensible Languages, Open Compilers,
and Reflection



October 21
Ray Mooney
Learning to Process Natural Language Using
Inductive Logic Programming



October 28
Mike Dahlin
Distributed I/O: from Clusters to Internets




November 4
Gordon Novak
Software Reuse by Specialization of Generic
Procedures through Views



November 11
Vijaya Ramachandran
The Design and Evaluation of Parallel
Algorithms



November 18
Lorenzo Alvisi
Lighweight fault-tolerance



November 25
Calvin Lin
Adaptive Libraries and High Level Optimization



December 2

Greg Plaxton
Analysis of Algorithms








Lighweight fault-tolerance
Lorenzo Alvisi

Distributed systems have moved beyond the confines of academia and
research labs and are revolutionizing the way in which businesses,
governmental organizations, and simple citizens are processing and
collecting information. Current technological trends promise to
dramatically increase the pace of this revolution by enabling the
design of highly cooperative distributed applications that go beyond
the client-server paradigm to harness the computational power of
distributed systems.

In this new environment, the scope and emphasis of fault-tolerant
techniques are about to undergo dramatic changes. Fault-tolerance will
cease to be an expensive feature required by a handful of applications
to tolerate exotic failures.  To the users of a highly distributed
information infrastructure, fault-tolerance will translate to a
competitive advantage, guaranteeing reliable access to critical
information.

In this talk we will explore a new way to design and engineer
fault-tolerant solutions, which we call lightweight fault-tolerance.
The goals of lightweight fault-tolerance are:

To require few dedicated resources and have a negligible
impact on performance during failure-free executions.
To scale its cost depending on the severity and number of
failures that need to be tolerated.
To integrate with applications in a way transparent to the
application programmer.
To enable and support emerging applications that will communicate
through messages as well as files.
To address software-generated faults effectively.





Cilk and Cilk-NOW: Adaptive and Reliable Parallel Computing on
Networks of Workstations
Robert Blumofe

This presentation overviews Cilk (pronounced
silk), an algorithmic parallel multithreaded language, and
Cilk-NOW, a runtime system that supports a functional
subset of Cilk on networks of workstations.  Cilk-NOW provides
adaptive parallelism and fault tolerance tranparently to
user programs.  Adaptive parallelism means that the set of
workstations on which a Cilk program runs can grow and shrink
dynamically depending on the availability of idle workstations and on
the amount of parallelism within the program.  In addition, a Cilk
program can continue execution even if one or more of its workstations
crashes, because the Cilk-NOW runtime system automatically detects and
recovers from such failures.  The presentation includes a live
demonstration.




Distributed I/O: from Clusters to Internets
Mike Dahlin

This presentation gives an overview of current issues in
distributed file system I/O. Technology trends and new applications
motivate more aggressive cluster and wide area network I/O systems. In
clusters, fast networks allow machines to cooperate closely to service
I/O requests. The xFS file system uses close cooperation among nodes
to provide better performance and availability than a single central
server. In wide area networks, the challenge is to provide good
performance, availability, and consistency despite limited network
performance and node or network failures. The wFS file system project
will explore these issues.




The Spatial Semantic Hierarchy for Human and Robot Cognitive Maps

Benjamin Kuipers

Human cognitive maps rely on several different representations for
large-scale space, each with its own ontology.  Similarly, a variety
of different approaches have been proposed for robot exploration and
mapping of unknown environments.  We cast these diverse
representations into a natural structure that we call the Spatial
Semantic Hierarchy (SSH), in which the objects, relations, and
assumptions at each level are abstracted from the levels below.

Each level of the SSH has its own mathematical foundation.  The
control level allows the robot and its environment to be formalized as
a continuous dynamical system, whose stable equilibrium points can be
abstracted to a discrete set of distinctive states.
Trajectories linking these states can be abstracted to actions, giving
a discrete causal graph representation of the state space.  The causal
graph of states and actions can in turn be abstracted to a topological
network of places and paths.  Local metrical models, such as occupancy
grids, of neighborhoods of places and paths can then be built on the
framework of the topological network without their usual problems of
global consistency.




Mathematical Principles of Logic Programming
Vladimir Lifschitz

Logic programming, as well as its sister approach, functional
programming, is based on the view that a computer program does not
need to contain any explicit operational instructions.  Instead, it
can simply provide a set of facts about the problem that is sufficient
to solve it.  Such a declarative program can be executed
using methods of automated reasoning.  Prolog is the best known logic
programming language.

The mathematical theory of logic programming is concerned with
defining the semantics of logic programming languages, describing the
reasoning algorithms used to implement them, and investigating the
soundness of these algorithms.




Adaptive Libraries and High Level Optimization
Calvin Lin

This talk describes a new approach to building software libraries.
By making libraries that can adapt---in both their implementation and
their interfaces---to different application needs and different
hardware platforms, we can produce libraries that are more efficient
and more widely usable.  We describe a new framework for building such
libraries, we describe three planned experiments that apply these
techniques to libraries for parallel scientific computation, and we
explain how our approach facilitates high level optimizations.




Learning Sequential Decision Tasks Through Symbiotic Evolution of
Neural Networks
Risto Miikkulainen

A novel reinforcement learning method called SANE
(Symbiotic, Adaptive Neuro-Evolution) evolves a population of neurons
through genetic algorithms to form a neural network for a given task.
Symbiotic evolution promotes both cooperation and specialization in
the population, which results in a fast, efficient genetic search and
discourages convergence to suboptimal solutions.  SANE is able to
extract domain-specific information even under sparse reinforcement,
which makes it an effective approach to a broad range of sequential
decision tasks such as robot control, game playing, and resource
management.




Alamo: The Net as a Data Warehouse
Dan Miranker

The Alamo effort is directed at intra-net development, and
inter-net users who can enumerate interesting sites and data
sources. The goal is to integrate the data sources and provide the
user with the illusion of a single virtual database, followed by
query, analysis and presentation tools.

Central to the Alamo architecture is a software bus called the
Abstract Search Machine (ASM). The ASM is a CORBA compliant interface
that provides a uniform interface to heterogeneous data
sources. Beyond simple data access, the ASM embodies a higher level of
abstraction enabling the efficient coding of clever search algorithms
and separating and isolating system concerns, including buffering and
data prefetch.

The broad claim is that high performance, often optimal,
implementations of advanced database facilities such as an
object-oriented query engine, a deductive inference engine, an active
database engine and data mining facilities can all be constructed
using the ASM as a common interface.

Finally, since the output of each of these advanced database
facilities can themselves serve as data sources, the components of the
Alamo architecture can be composed to resolve higher level data
integration problems. In particular we anticipate using the elements
of Alamo itself to represent meta-data and resolve both structural and
semantic conflicts among the data sources. Ultimately, further
compositions will embody complex knowledge-bases and be able to answer
high-level queries.




Learning to Process Natural Language Using Inductive Logic
Programming
Raymond J. Mooney

Inductive Logic Programming (ILP) addresses the problem of learning
Prolog programs from examples.  The representational power of
first-order logic offers advantages over standard machine learning
methods constrained to use fixed-length feature vectors.  We are
applying ILP methods to natural-language learning where we believe
this richer representation offers important advantages.  We have
developed an ILP system, CHILL, for learning deterministic parsers
from a corpus of parsed sentences. CHILL obtains superior results on
several artificial corpora previously used to test neural-network
methods, and encouraging results on the more realistic ATIS corpus of
airline queries.  CHILL has also been used to the automatically
develop a complete natural-language interface that translates English
database queries into executable Prolog form, producing a more
accurate parser than a hand-built system for querying a small
geographic database.  We have also developed an ILP system, FOIDL,
which has been applied to learning the past tense of English,
surpassing the previous results of neural-network and decision-tree
methods on this problem.




Software Reuse by Specialization of Generic Procedures through
Views
Gordon S. Novak Jr.

Software reuse is clearly a good idea, but it is difficult to
achieve in practice: if your data does not fit the assumptions of the
software, reusing the software will be difficult.  In our approach,
views describe how application data types implement the abstract types
used in generic procedures.  A compilation process can specialize a
generic procedure to produce a version that is customized for the
application data.  Graphical user interfaces make it easy to specify
views.  An Automatic Programming Server has been implemented on the
World Wide Web; it will write specialized programs for the user, in a
desired language, and serve the source code to the user as a file.




Analysis of Algorithms

Greg Plaxton

A major focus of theoretical computer science is the design and
analysis of asymptotically efficient algorithms
(sequential/parallel/distributed, deterministic/randomized) for
specific computational problems.  In this research area, it is not
uncommon to come across well-written papers in which, informally: (i)
the main underlying ideas are conceptually straightforward, (ii) the
formal presentation is surprisingly lengthy, and (iii) most of the
formalism deals with minor side-issues and special cases that have
little or nothing to do with the main underlying ideas.  In such
papers, there seems to be a significant gap between the conceptual and
formal difficulty of the algorithm being presented.  Are such gaps
inherent, or is conventional mathematical notation simply inadequate
for succinctly formalizing certain conceptually straightforward
algorithmic ideas?

In this talk, I will describe a notation for asymptotic analysis,
called $O_i$-notation, that significantly reduces the
conceptual-to-formal gap associated with a non-trivial
class of algorithms.  As a concrete example, I consider the analysis
of the well-known linear-time selection algorithm due to Blum, Floyd,
Pratt, Rivest, and Tarjan.




The Design and Evaluation of Parallel Algorithms
Vijaya Ramachandran

The design and analysis of efficient parallel algorithms for
combinatorial problems has been an area of extensive study in recent
years, and a large number of algorithms have been developed on the
abstract PRAM model of parallel computation.  In this talk we will
describe some of our work in the design of efficient parallel
algorithms, and our experience with implementing and evaluating these
algorithms on a massively parallel machine (Maspar MP-1). We will then
describe a queuing variant of the PRAM model, which we
propose as a more appropriate model for currently available parallel
shared-memory machines than traditional PRAM models.




Extensible Languages, Open Compilers, and Reflection
Paul Wilson

Extensible languages allow interesting new features to be added to
a language portably, from within the language itself.

Open compilers allow fairly easy modification of compilers to add
new features, analyses, and optimizations.

Reflection allows a program to examine a representation of
interesting parts of itself, and affect its own structure
accordingly.

I'll discuss these things, why they're useful for building modular,
portable, and adapatable software.  I'll also discuss our recent work
on the RScheme compiler, an open compiler for an extensible
language.





Last modified: November 15, 1996
Robert Blumofe
rdb@cs.utexas.edu



